Metadata-Version: 2.1
Name: brix.data-integration
Version: 0.1.11
Summary: Data integration utility
Author: QuantumBlack Labs
Requires-Python: >=3.8
Description-Content-Type: text/markdown
Requires-Dist: pyspark <4.0,>=3.1
Requires-Dist: pandas <2.0,>=1.0
Provides-Extra: all
Requires-Dist: brix.data-integration[test] ; extra == 'all'
Provides-Extra: test
Requires-Dist: flake8 <4.0,>=3.5 ; extra == 'test'
Requires-Dist: isort <5.0,>=4.3.16 ; extra == 'test'
Requires-Dist: pytest >=7.0 ; extra == 'test'
Requires-Dist: pytest-cov >=2.5 ; extra == 'test'
Requires-Dist: pydocstyle <7.0.0,>6.0.0 ; extra == 'test'
Requires-Dist: pylint ==2.11.1 ; extra == 'test'
Requires-Dist: click <8.0,>=7.0.0 ; extra == 'test'
Requires-Dist: pytest-mock ~=1.7 ; extra == 'test'
Requires-Dist: yamllint ==1.25 ; extra == 'test'
Requires-Dist: chispa ==0.8.3 ; extra == 'test'
Requires-Dist: tabulate >=0.8.9 ; extra == 'test'

# Data integration package

**Easily ingest historical or incremental raw data into C1W. In
addition, make use of data cleaning and transformation
functions using flexible declarations.**

The data-ingesting package allows you to:

- Integrate historical or incremental raw data to analytics pipelines, as a first layer
  in the raw/landing zone.

- Enables the creation of partition columns, if not present in the input data. This
  capability allows selective read in downstream processing and makes data pipelines more
  efficient.

!!! warning

    - It should NOT be used to create new features but only to transform existing data
      so it can be integrated to the primary data model.

    - It should not be used if data can be filtered on the source before receiving. In this
      case would be better to just feed directly to books, and avoid extra storage in
      this layer level.

## Package structure

```
data-integration/
   ├── README.md
   ├── RELEASE.md
   ├── data_integration
   │   ├── core
   │   │   ├── processor.py             # (1)
   │   │   └── utils
   │   │       ├── date_validation.py   # (2)
   │   │       ├── ingestion.py         # (3)
   │   │       └── transformations.py   # (4)
   │   ├── nodes
   │   │   └── ingestor.py              # (5)
   │   └── tests                        # (6)
   ├── pyproject.toml
   ├── requirements.txt
   └── test_requirements.in

```

1. Processor class used to pass and validate data.
2. Internal date validation functionalities.
3. Main ingestion functionalities.
4. Main transformation functionalities.
5. Wrapper that serves as entry point to package.
6. Unit test cases.

## General overview
The execution entrypoint of this package are the
`data_transformation.nodes.ingestor` methods
that expects to receive a raw input dataframe and a "recipe"-like dictionary,
containing a list of instructions with the intended operations that must be applied.

The following sections cover both main functionalities of this package
which are **data ingestion** and **data transformation**, that are usually used in this
order.

### Data ingestion (historical and incremental)

#### Motivation

When working with large volume data, being able to process only new data is critical.
Millions of data points could be received daily e.g. when working with transaction
level data. Processing all data in every run becomes inefficient and unnecessary.

There are cases where raw data is not saved in efficient file formats like parquet
and are not partitioned making it difficult to only load the data needed for processing.

By having this package, in situations like mentioned above, we can save only new rows
of information (new data) and save it partitioned (format given by user) in efficient
file formats like parquet.

This enables the possibility to load only a subset of data, the new unprocessed data in
an incremental fashion work and process it to append to the feature store layer and be
ready and available for analytics use cases.

#### Examples of use

##### Historical

While running pipeline for the first time you may want to process historical data to
initially populate feature store for first time training.
the module `ingestor` is our entry point to the package. For historical you would call
in a node like the following example:

!!! example "Historical ingestion example"

    === "nodes.yaml"

        ```yaml
        data_ingestion_historical:
          func: data_integration.nodes.ingestor.ingest_historical_data
          inputs:
            input_parameters_dict: parameters.data_ingestion_historical_example1
            raw_df: dbfs:/mnt/location/of/data/credit_card_transaction
          outputs:
            catalog.credit_card.transaction_ingestion@_overwrite
        ```

    === "parameters.yaml"

        ```yaml
        data_ingestion_historical_example1:
          col_to_filter_by: transaction_dt
          date_col_by_expr:
            year: date_format(transaction_dt, 'y')
          start_dt: 2015-01-01
          end_dt: 2019-12-12
        ```

    === "catalog.yaml"

        ```yaml
        _spark_parquet: &_spark_parquet
          type: spark.SparkDataSet
          file_format: parquet

        credit_card.transaction_ingestion@input:
          <<: *_spark_parquet
          filepath: ${globals.ingestion_dir_path}/credit_card/transaction_ingestion

        credit_card.transaction_ingestion@_overwrite:
          <<: *_spark_parquet
          filepath: ${globals.ingestion_dir_path}/credit_card/transaction_ingestion
          save_args:
            mode: overwrite
            partitionBy:
              - year

        credit_card.transaction_ingestion@_append:
          <<: *_spark_parquet
          filepath: ${globals.ingestion_dir_path}/credit_card/transaction_ingestion
          save_args:
            mode: append
            partitionBy:
              - year
        ```

In the previous example, we will ingest "catalog.raw_zone_df1" and process it through
the following steps:


1. Using the input parameters, we traverse `date_col_by_expr` and create the columns
  `joining_year` and `custom_operation` using the corresponding expression of the
  `key:value` pair.

2. Through setting the parameter `col_to_filter_by: joining_year` we are passing the
  previously created `joining_year` date column to be first checked if it is of type date
  if not try to convert it to date. Then if `start_dt` and `end_dt` are set,
  `col_to_filter_by` column will be used to slice the data in accordance with filtering
  parameters.

3. Filtering through the use of start_dt and end_dt parameters we slice data we want
  to ingest in this first run.

4. Making use of of the catalog we are able to use the option to write as "append mode" of
  [Kedro Catalog option](https://kedro.readthedocs.io/en/stable/kedro.extras.datasets.spark.SparkDataSet.html#kedro.extras.datasets.spark.SparkDataSet.__init__),
  we are able to "append" data to the output dataset and make use of the partition
  parameter and pass the partition columns we may want our data to be saved to disk.
  Newly created `joining_year` is used in this example as column to partition the data
  on disk.

##### Incremental

Assuming we already did a first historical data integration previously, you may want to
append new data to the dataset, in order to complement the existing data and use the
complete set for model training or prediction.

!!! example "Incremental ingestion example"

    === "nodes.yaml"

        ```yaml
        data_ingestion_incremental:
        func: data_integration.nodes.ingestor.ingest_incremental_data
        inputs:
          input_parameters_dict: parameters.data_ingestion_incremental_example1
          # example of client data stored in blob storage mounted on databricks
          raw_df: dbfs:/mnt/zcae070001/data/credit_card_transaction
          existing_data_df: catalog.credit_card.transaction_ingestion@input
        outputs:
          catalog.credit_card.transaction_ingestion@_append
        ```

    === "parameters.yaml"

        ```yaml
        data_ingestion_incremental_example1:
        col_to_filter_by: transaction_dt
        date_col_by_expr:
          year: date_format(transaction_dt, 'y')
        ```

    === "catalog.yaml"

        ```yaml
        _spark_parquet: &_spark_parquet
          type: spark.SparkDataSet
          file_format: parquet

        credit_card.transaction_ingestion@input:
          <<: *_spark_parquet
          filepath: ${globals.ingestion_dir_path}/credit_card/transaction_ingestion

        credit_card.transaction_ingestion@_overwrite:
          <<: *_spark_parquet
          filepath: ${globals.ingestion_dir_path}/credit_card/transaction_ingestion
          save_args:
            mode: overwrite
            partitionBy:
              - year

        credit_card.transaction_ingestion@_append:
          <<: *_spark_parquet
          filepath: ${globals.ingestion_dir_path}/credit_card/transaction_ingestion
          save_args:
            mode: append
            partitionBy:
              - year
        ```

In the previous example, we will ingest "catalog.raw_zone_df1" and process it through
the following steps:

1. Using the input parameters, we traverse `date_col_by_expr` and create the columns
  `joining_year` and `custom_operation` using the corresponding expression of the
  `key:value` pair.

2. Through setting the parameter `col_to_filter_by: joining_year` we are passing the
  previously created `joining_year` date column to be first checked if it is of type date
  if not try to convert it to date.

3. We will then find the nearest date available in the `col_to_filter_by` column to
  append only rows which are after said date in the already existing dataframe.

4. Making use of of the catalog we are able to use the option to write as "append mode" of
  [Kedro Catalog option](https://kedro.readthedocs.io/en/stable/kedro.extras.datasets.spark.SparkDataSet.html#kedro.extras.datasets.spark.SparkDataSet.__init__),
  we are able to "append" data to the output dataset and make use of the partition
  parameter and pass the partition columns we may want our data to be saved to disk.
  Newly created `joining_year` is used in this example as column to partition the data on
  disk.

### Data transformation

#### Expected inputs

To allow easy evolution and addition of new transformations, the
`core.processor.transform()` method has a stable programming interface with
the `utils/transformations.py` transformations. The requirements are:

##### A dataframe
The supported types are:

|Type |Class |
|------|------|
|Pandas dataframe |`pandas.DataFrame` |
|Pyspark dataframe |`pyspark.sql.DataFrame` |

##### A list of instructions
The list of instructions to be applied should be:

- A list of dataframe columns names, to where the transformation will be applied, or
- A two-level deep python dictionary, containing the instructions and
configurations to be applied.

The declared order of these instructions in the dictionary is not important, as the
`transform()` function applies its own defined order intended to have three stages:

- Initially makes column names and types easier to work with, through renaming/casting.
- Deal with string values, through replacement/trimming.
- Fill missing values.
- Apply *very* flexible SQL expressions to handle what basic casting operations can't
  handle.
- Clean the final output, dropping rows/columns.


    !!! tip

        Check the warning section about
        [Apply SQL expressions](#####Apply_SQL_expression).

The **predefined** transformation order to be executed is defined under the hardcoded
`default_transform_operations` dictionary inside `core.processor.transform()` and
is replicated in the table below:

|Transformation |Description |
|------|------|
|[rename_columns](#rename-columns) |Rename columns to make them more recognizable |
|[cast_columns](#cast-columns) |Cast columns to the desired types |
|[trim_columns](#trim-columns) |Trim some textual columns to remove empty initial and ending spaces |
|[trim_all_string_columns](#trim-all-string-columns) |Trim all textual columns to remove empty initial and ending spaces |
|[regex_replace_values](#regex_replace_values) |Replace part of string or entire string values, according to a matching regular expression |
|[fill_na](#fill-na) |Fill missing values in a subset of columns |
|[apply_sql_expression](#apply-sql-expression) |Apply SQL expression *¹ |
|[drop_duplicates](#drop-duplicates) |Drop duplicates rows from a subset of columns |
|[drop_null_values](#drop-null-values) |Drop null values from a subset of columns |
|[drop_columns](#drop-columns) |Drop unwanted columns |
|[keep_columns](#keep-columns) |Keep wanted columns only |
|[drop_rows](#drop-rows) |Drop unwanted rows |
|[keep_rows](#keep-rows) |Keep wanted rows only |

###### Rename columns
Renames the specified list of columns to their new names:
```py
"rename_columns": {
    "customer-id": "customer_id",
    "birth_dt": "date_of_birth",
}
```

!!! note "Side-effect of this instruction"

    The renamed columns are moved to the end of the dataframe, as the column is removed
    and recreated.

If the old and the new column names are the same, the name will be kept.

The columns not specified in the instructions will be kept.

###### Cast columns
Cast the specified list of columns to their new types using python types:
```py
"cast_columns": {
    "customer_id": "str",
    "joining_dt": "date",
    "risk_value": "int",
}
```

!!! note "Side-effect of this instruction"

    The renamed columns are moved to the end of the dataframe, as the column is removed
    and recreated.

Conversion table used internally, mapping python types to corresponding Pyspark
`DataType`s:

|Python type as the input |Resulting column type |
|---|---|
|`str` |`StringType()` |
|`bool` |`BooleanType()` |
|`int` |`IntegerType()` |
|`long` |`LongType()` |
|`float` |`FloatType()` |
|`double` |`DoubleType()` |
|`bytearray` |`BinaryType()` |
|`bytes` |`BinaryType()` |
|`decimal` |`DecimalType()` |
|`date` |`DateType()` |
|`datetime` |`TimestampType()` |
|`time` |`TimestampType()` |
|`timestamp` |`TimestampType()` |
|`none` |`NullType()` |

Complex types like `ArrayType()`, `MapType()` and `StructType()` are not currently
supported.

###### Trim columns
Remove empty or unknown characters from the beginning and the ending of a `StringType`
column.

!!! warning "About datatypes"

    Trim operations should not be used with other types of columns beside **StringType**,
    otherwise the column would be converted to `StringType` after the operation.
    There is a validation step before the conversion, to check the type of all columns.
    If at least one of them is not `StringType`, an exception will be thrown.

```py
"trim_columns": ["nationality", "purchased_category",]
```

###### Trim all string columns
Remove empty or unknown characters from the beginning and the ending of every
`StringType` column.

```py
"trim_all_string_columns": None
```

!!! question "What about `trim_columns`?"

    This function makes use of the already existing `trim_columns` function but its
    main purpose is to automatically apply the `trim_columns` transformation in every
    `StringType` column of the dataframe, without the need to specify them one by one.

    You can consider this operation a shortcut to:

    1. Discover all `StringType` columns of the dataframe;
    1. Make a list with them;
    1. Pass the list in the `trim_columns` transformation.

    ```py
    "trim_columns": [ <lots_of_manually_defined_string_columns> ]
    ```

!!! note "About `None` as value"

    To allow easy plugging of new transformations, the interface between
    `processor.transform()` method and each transformation defined inside
    `transformations.py` is fixed: a dataframe **and** a list/dictionary containing
    **instructions** to be applied in the dataframe.

    In the case of `trim_all_string_columns`, there's no need to define any
    instruction (the list of `StringType` columns will be generated internally),
    so we need to set some value (`None` was chosen) to characterize the entire
    second argument `instructions` as a dictionary.

    As the `None` value, if another value is passed it will be ignored.

###### regex_replace_values
Replace **entire values** or **partial values** in columns, according to a matching regular expression
and a new value to replace the original one.

```py
"columns_dict_full" : {'col1': {'bar': 'yyy'},
                'col2': {'\\d+': 'full'},
                'col3': {'\\d+': 'testing'}}

regex_replace_values(df, columns_dict_full, True).show()

"columns_dict_partial" : {'col1': {'foo': 'xxx',
                                             r"[^a-zA-Z0-9_.-]":"",
                                            'bar': 'yyy'},
                                    'col2': {'\\d+': 'num'},
                                    'col3': {'\\d+': '***'}}
regex_replace_values(df, columns_dict_partial, False).show()
```

The evaluation is done in the order defined in the dictionary:
for each row of the column, the first regular expression in the dictionary to match
the value will be applied and the subsequent matching expressions in the dictionary
will be handled for that row.
A function that replaces substrings in specified columns of a PySpark DataFrame
based on regular expression patterns provided in a dictionary. The replacement
can be partial (i.e. only replace matched substring) or full string replacement.

###### Fill NA
Fill missing values in a subset of columns:
```py
"fill_na": {
    "update_dt": "2020-01-01 00:00:00",
    "purchased_amount": 0,
}
```

###### Apply SQL expression
Apply a Spark SQL expression to an existing column.

The expected format of the SQL expression is restricted to _select-like_ operations,
like the examples below and not a full SQL query with all its statements
(`WHERE`, `GROUP BY`, etc):
```py
"apply_sql_expression": {
    "joining_year": "date_format(joining_dt, 'y')",
    "custom_operation": "date_format(date_add(to_date(birth_dt), 10), 'yyyy-MM-dd')",
}
```

This is similar to:
```sql
SELECT date_format(joining_dt, 'y') AS joining_year,
       date_format(date_add(to_date(birth_dt), 10), 'yyyy-MM-dd') AS custom_operation
...
```

!!!Warning About "Apply SQL expressions"

    This _very_ flexible operation is meant to enable some kind of casting when a
    `date` format has peculiar formats.

    For example, casting a `string` value in a `date` column where the day value is not
    present and the date format `yyyyMM` must be explicitly set during the `to_date()`
    operation:

    |end_date|
    |--------|
    |202208  |

    Using only the `cast_columns` instruction, this task would be impossible as it
    would rely on a simple `f.col("end_dt").cast(DateType())` instruction, resulting in
    an error (due to the missing `day` part of this date).

    Applying a SQL-like expression to this column enables the conversion, as the day
    `01` will be used by default:
    ```sql
    TO_DATE(end_dt, 'yyyyMM')
    ```

    But it also enables the user to create new columns during the transformation:
    ```python
    "apply_sql_expression": {
        "new_column_to_be_created": "(colA + colB) * 1.5",
    }
    ```

    Such pattern shouldn't be used as the data transformation layer **shouldn't have
    this kind of business rules**.
    This kind of data manipulation should be done during the basic feature creation
    in another layer (using *books_framework package*), where you have even more
    flexibility to use pyspark code to deal with feature creation and where all
    business-related transformation should be applied.
    This *backdoor* was left open for any kind of **really exceptional necessity**
    during client deployments.

    **This instruction should be only used to do basic transformations on existing data
    and shouldn't be used to create new calculated data or new features.**

###### Drop duplicates
Drop rows which contains duplicate values in an optional `subset` of columns.
```py
"drop_duplicates": ["joining_dt"],
```

###### Drop null values
Drop rows which contains `null` values in an optional `subset` of columns.
```py
"drop_null_values": ["update_dt"]
```

###### Drop columns
Drop a list of existing columns of the dataframe.
```py
"drop_columns": ["nationality", "taxes_paid"]
```

An `assert` error will be raised if some column(s) of the list doesn't exist in the
dataframe.

Usually this instruction shouldn't be used together `keep_columns`, as this one should
be chosen when the list of columns for dropping is smaller than the list of columns
to keep. Anyway, no limitation is defined and both can be used.

###### Keep columns
Keep a list of existing columns of the dataframe.
```py
"keep_columns": ["customer_id", "joining_dt"]
```

An `assert` error will be raised if some column(s) of the list doesn't exist in the
dataframe.

Usually this instruction shouldn't be used together `drop_columns`, as this one should
be chosen when the list of columns for keeping is smaller than the list of columns
to drop. Anyway, no limitation is defined and both can be used.

###### Drop rows
Used for dropping all rows of the dataframe that matches the conditionals defined in
Spark SQL expressions.

Every expression given must return a `True` or `NULL` value, so a `CASE/WHEN/ELSE`
pattern is the most common kind of expression to be used:
```py
"drop_rows": {
    "drop_invalid_update_dt": "CASE WHEN update_dt IS NULL THEN True ELSE NULL END",
    "drop_customers_end_2": "CASE WHEN customer_id LIKE '%2' THEN True ELSE NULL END",
}
```

!!! info "Important"

    The *false* return values of the expression **must be `NULL`**, so the `drop_rows`
    can work accordingly and consider them as a non-matching row.

    Setting `False` value will result in the drop of wrong rows.

Usually this function shouldn't be used together with `keep_rows`, as this one
should be chosen when the list of expressions or the complexity for dropping is
smaller/lesser than the list of conditions or complexity to keep other rows.
Anyway, no limitation is defined and both can be used.

###### Keep rows
Used for keeping all rows of the dataframe that matches the conditionals defined in
Spark SQL expressions.

Every expression given must return a `True` or `NULL` value, so a `CASE/WHEN/ELSE`
pattern is the most common kind of expression to be used:
```py
"keep_rows": {
    "keep_invalid_update_dt": "CASE WHEN update_dt IS NULL THEN True ELSE NULL END",
    "keep_customer_c001": "CASE WHEN customer_id = 'c001' THEN True ELSE NULL END",
}
```

!!! info

    The *false* return values of the expression **must be `NULL`**, so the `keep_rows`
    can work accordingly and consider them as a non-matching row.

    Setting `False` value will result in the keep of wrong rows.

Usually this function shouldn't be used together with `drop_rows`, as this one
should be chosen when the list of expressions or the complexity for keeping is
smaller/lesser than the list of conditions or complexity to drop other rows.
Anyway, no limitation is defined and both can be used.

###### Applying more than one instruction
More than one operation can (and should!) be defined inside the same
transformation node.

The above topics gave detailed examples of each individual instruction but usually they
will be concatenated into a single instruction dictionary.

Being a dictionary, various instructions can be used and the later instructions must
consider the changes already done in the previous ones:
```py
"keep_rows": {
    "keep_invalid_update_dt": "CASE WHEN update_dt IS NULL THEN True ELSE NULL END",
    "keep_customer_c001": "CASE WHEN customer_id = 'c001' THEN True ELSE NULL END",
},
"rename_columns": {
    "customer_id": "customer_id",
    "birth_dt": "date_of_birth", # renaming from `birth_date` to `date_of_birth`
},
"apply_sql_expression": {
    "joining_year": "date_format(joining_dt, 'y')",
    "custom_operation": "date_format(date_add(to_date(date_of_birth), 10), 'yyyy-MM-dd')", # using the renamed column here
}
```

In the example above, `apply_sql_expression` is executed after `rename_columns` and it
uses the already renamed `date_of_birth` column because it was already renamed
in the previous operation.

!!! info "To remember"

    The order of the instructions is not defined by the user.

#### Usage patterns

Depending on how the client's data is stored, you can have two distinct usage patterns
that gets you closer to the intended C1W's Data Model.

##### One input, one output
For data that is already correlated or under the same domain.
For example: a dataset with only credit card transactions, a dataset with only loan
taxes etc. In this case, the data doesn't need to be segregated in different domains,
as it already is.

You can use just one kedro node to execute the
`data_transformation.nodes.ingestor.transform_data()` method on some input data and transform it
using simple transformations:
output:
```{mermaid}
graph LR
    input["input domain<br>data"]
    outputA["output domain<br>data"]
    input--"nodes.ingestor.transform_data()"-->outputA
```

##### One input, multiple outputs
For data that is mixed up or contains information that belongs to different business
domains and should be separated.
For example: one dataset containing credit_card transactions, categories and customer
details.
In this case, the data needs to be segregated in different domains to match the
C1W's Data Model.

You can use multiple kedro nodes to execute the
`data_transformation.nodes.ingestor.transform_data()` method on the **same** input data,
generating multiple outputs:
```{mermaid}
graph LR
    input["lots of<br>mixed data"]
    outputA["some<br>related data"]
    outputB["another<br>related data"]
    outputC["yet another<br>related data"]
    input--"nodes.ingestor.transform_data()"-->outputA
    input--"nodes.ingestor.transform_data()"-->outputB
    input--"nodes.ingestor.transform_data()"-->outputC
```

#### Debugging the generated execution plan

Being built over PySpark, `data_integration` has the capability of outputing the 
underlying Spark execution plans to allow investigation on performance bottlenecks 
during its processing, like single partitioning processing, 
incorrectly partitioned files, data skewness etc.

There is one option available that sends the output as a 
`DEBUG` logging entry, that can be captured/handled by configured loggers/handlers:

- To get the 
**complete execution plan from the final DataFrame that will be generated**, 
send an additional parameter `log_spark_info = True` to 
`data_integration.v0.core.processor.Processor.transform()`:

    Through python code:
    ```py
    Processor.transform(
        raw_df=..., 
        instructions=..., 
        log_spark_info=True
    )
    ```

    Through `yaml` configuration on Kedro nodes:
    ```yaml
    # nodes.yaml
    data_transformation:
      func: data_integration.v0.nodes.ingestor.transform_data
      inputs:
        raw_df: input.catalog.persona.customer@spark
        instructions: parameters.data_transformation_instructions
        log_spark_info: parameters.log_spark_info
      outputs:
        catalog.persona_customer@spark

    # parameters.yaml
    log_spark_info: true
    ```

The expected output is similar to:
```text
...
DEBUG:data_integration.v0.core.utils.spark_info Number of RDD partitions: 1
DEBUG:data_integration.v0.core.utils.spark_info Generated Spark plan:

## Extended:
== Parsed Logical Plan ==
Project [postcode#4, marital_status#5, annual_income#6, dependents_count#7L, nb_cars#8L, nb_houses#9L, risk_score#10L, gender#12, profession#13, nature_of_customer#15, internal_customer_id#16, first_name#17, last_name#18, permanent_address_location#19, current_address_location#20, billing_address_location#21, highest_education_level#22, is_parent#23L, employment_type#24, occupation_type#25, is_homeowner#26L, household_income_range#27, total_personal_expense_amt#28, credit_score#29L, ... 11 more fields]
+- Filter isnull(final_flag_14b093#416)
[...]

(1) Scan parquet 
Output [14]: [_id#11655, _observ_end_dt#11656]
Batched: true
Location: InMemoryFileIndex [file:/data/base/raw/customer]
ReadSchema: struct<_id:string,...>

(2) Exchange
[...]
```
