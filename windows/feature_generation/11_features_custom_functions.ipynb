{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bd2982",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"../\")\n",
    "sys.path.insert(0, \"packages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511c879e",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if os.environ.get(\"CIRCLECI\"):\n",
    "    default_env = os.environ.get(\"CONDA_DEFAULT_ENV\")\n",
    "    os.environ[\"PYSPARK_DRIVER_PYTHON\"] = (\n",
    "        f\"/home/circleci/miniconda/envs/{default_env}/bin/python\"\n",
    "    )\n",
    "    os.environ[\"PYSPARK_PYTHON\"] = (\n",
    "        f\"/home/circleci/miniconda/envs/{default_env}/bin/python\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9019439",
   "metadata": {},
   "source": [
    "# Custom Window Functions\n",
    "\n",
    "This document outlines how to create features using the available custom window functions via examples.\n",
    "\n",
    "The custom window sub-module aims to create window features with custom user-defined functions, that can be applied with `generate_window_grid` or `window_column`.\n",
    "\n",
    "We will be using the following dataframe throughout our examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6293ba5",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import (\n",
    "    DateType,\n",
    "    IntegerType,\n",
    "    StringType,\n",
    "    StructField,\n",
    "    StructType,\n",
    ")\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.config(\"spark.ui.showConsoleProgress\", False)\n",
    "    .config(\"spark.sql.shuffle.partitions\", 1)\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"date\", DateType(), True),\n",
    "        StructField(\"date_index\", IntegerType(), True),\n",
    "        StructField(\"y_flag\", IntegerType(), True),\n",
    "        StructField(\"x_flag\", IntegerType(), True),\n",
    "    ]\n",
    ")\n",
    "data = [\n",
    "    (\"Gendry\", pd.Timestamp(\"2012-05-01\").date(), 15461, 1, 2),\n",
    "    (\"Gendry\", pd.Timestamp(\"2012-05-02\").date(), 15462, 0, None),\n",
    "    (\"Gendry\", pd.Timestamp(\"2012-05-03\").date(), 15463, 1, None),\n",
    "    (\"Gendry\", pd.Timestamp(\"2012-05-04\").date(), 15464, 0, 1),\n",
    "    (\"Gendry\", pd.Timestamp(\"2012-05-05\").date(), 15465, 1, None),\n",
    "    (\"Arya\", pd.Timestamp(\"2012-05-06\").date(), 15466, 1, 0),\n",
    "    (\"Arya\", pd.Timestamp(\"2012-05-07\").date(), 15467, 0, None),\n",
    "    (\"Arya\", pd.Timestamp(\"2012-05-08\").date(), 15468, 0, 1),\n",
    "    (\"Arya\", pd.Timestamp(\"2012-05-09\").date(), 15469, 1, None),\n",
    "    (\"Arya\", pd.Timestamp(\"2012-05-10\").date(), 15470, 1, 2),\n",
    "    (\"Cersei\", pd.Timestamp(\"2012-05-10\").date(), 15470, 0, 3),\n",
    "    (\"Cersei\", pd.Timestamp(\"2012-05-11\").date(), 15471, 1, None),\n",
    "    (\"Cersei\", pd.Timestamp(\"2012-05-12\").date(), 15472, 1, None),\n",
    "    (\"Cersei\", pd.Timestamp(\"2012-05-13\").date(), 15473, 1, None),\n",
    "    (\"Cersei\", pd.Timestamp(\"2012-05-15\").date(), 15475, 0, 1),\n",
    "]\n",
    "df_window = spark.createDataFrame(data, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafb871d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_window.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bdb28b",
   "metadata": {},
   "source": [
    "The current list of custom window functions comprises 2 main functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cc7402",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from feature_generation.v1.core.features import custom_windows\n",
    "import inspect\n",
    "\n",
    "\n",
    "module = custom_windows\n",
    "ignore_prefix = \"_\"\n",
    "allow_list = []\n",
    "\n",
    "found_functions = []\n",
    "for func_name, func in inspect.getmembers(module):\n",
    "    is_function = inspect.isfunction(func)\n",
    "    is_defined_in_module = inspect.getmodule(func) == module\n",
    "    is_allowed = func_name in allow_list\n",
    "    is_not_ignored = (ignore_prefix == \"\") or not (func_name.startswith(ignore_prefix))\n",
    "    flag = (is_allowed) or (is_function and is_defined_in_module and is_not_ignored)\n",
    "    if flag:\n",
    "        found_functions.append((func_name, func))\n",
    "\n",
    "rows = []\n",
    "for func_name, func in found_functions:\n",
    "    x_doc = func.__doc__.split(\"\\n\")[0]\n",
    "    rows.append(\n",
    "        {\"function\": \"{}.{}\".format(module.__name__, func_name), \"description\": x_doc}\n",
    "    )\n",
    "\n",
    "from tabulate import tabulate\n",
    "import pandas as pd\n",
    "\n",
    "table = pd.DataFrame(rows)\n",
    "print(tabulate(table, headers=table.columns, tablefmt=\"psql\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e0c26c",
   "metadata": {},
   "source": [
    "## `complete_*` functions\n",
    "  \n",
    "These UDFs performs operations only if entire window range is complete.\n",
    "\n",
    "This is useful when computing forward-looking target variables (rangeBetween positive)\n",
    "or backward-looking features (rangeBetween negative), from continuous data sequences,\n",
    "that should only be computed for windows where all data entries are known\n",
    "(e.g., if in February we count the number of events X between January and June,\n",
    "the return value will be null because the data is not complete).\n",
    "\n",
    "It's intended to work based on defined `range_between`. For instance:\n",
    "\n",
    "Will work:\n",
    "  - complete_sum(2) & ranges_between = [[1, 2]]\n",
    "  - complete_sum(12) & ranges_between = [[1, 12]]\n",
    "  - complete_sum(6) & ranges_between = [[3, 8]]\n",
    "  - complete_sum(6) & ranges_between = [[-6, -1]]\n",
    "\n",
    "Won't work (meaningless results):\n",
    "  - complete_sum(2) & ranges_between = [[1, 3]]\n",
    "  - complete_sum(12) & ranges_between = [[1, 6]]\n",
    "  - complete_sum(6) & ranges_between = [[3, 12]]\n",
    "  - complete_sum(6) & ranges_between = [[-5, -1]]\n",
    "\n",
    "Note that input (spine) needs to be continuous historical data\n",
    "(e.g. no date gaps for each UOA).\n",
    "\n",
    "This is a known limitation at the moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7ada6a",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "from feature_generation.v1.core.features.create_column import create_columns_from_config\n",
    "from feature_generation.v1.core.features.windows import (\n",
    "    generate_window_grid,\n",
    "    window_column,\n",
    "    generate_windows_spec,\n",
    ")\n",
    "from feature_generation.v1.core.features.custom_windows import (\n",
    "    complete_sum,\n",
    "    complete_max,\n",
    ")\n",
    "\n",
    "windows_grid_code_config = [\n",
    "    # Features\n",
    "    generate_window_grid(\n",
    "        inputs=[\"x_flag\"],\n",
    "        funcs=[pyspark.sql.functions.sum, complete_sum(2, return_dtype=\"integer\")],\n",
    "        windows=[{\"partition_by\": [\"name\"], \"order_by\": [\"date_index\"]}],\n",
    "        ranges_between=[[-2, -1]],\n",
    "    ),\n",
    "    window_column(\n",
    "        outputs=\"x_flag_complete_sum_past_2_1_wc\",\n",
    "        input=complete_sum(2, return_dtype=\"integer\")(df_window[\"x_flag\"]),\n",
    "        windows_spec=generate_windows_spec(\n",
    "            partition_by=\"name\", order_by=\"date_index\", range_between=[-2, -1]\n",
    "        ),\n",
    "    ),\n",
    "    # Target\n",
    "    generate_window_grid(\n",
    "        inputs=[\"y_flag\"],\n",
    "        funcs=[pyspark.sql.functions.max, complete_max(2, return_dtype=\"integer\")],\n",
    "        windows=[{\"partition_by\": [\"name\"], \"order_by\": [\"date_index\"]}],\n",
    "        ranges_between=[[1, 2]],\n",
    "    ),\n",
    "    window_column(\n",
    "        outputs=\"y_flag_complete_max_next_1_2_wc\",\n",
    "        input=complete_max(2, return_dtype=\"integer\")(df_window[\"y_flag\"]),\n",
    "        windows_spec=generate_windows_spec(\n",
    "            partition_by=\"name\", order_by=\"date_index\", range_between=[1, 2]\n",
    "        ),\n",
    "    ),\n",
    "]\n",
    "\n",
    "df_windows_grid_code = create_columns_from_config(df_window, windows_grid_code_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fafd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_windows_grid_code.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
