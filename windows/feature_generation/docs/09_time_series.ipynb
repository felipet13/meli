{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f298def0",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"../\")\n",
    "sys.path.insert(0, \"packages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ffe000",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if os.environ.get(\"CIRCLECI\"):\n",
    "    default_env = os.environ.get(\"CONDA_DEFAULT_ENV\")\n",
    "    os.environ[\"PYSPARK_DRIVER_PYTHON\"] = (\n",
    "        f\"/home/circleci/miniconda/envs/{default_env}/bin/python\"\n",
    "    )\n",
    "    os.environ[\"PYSPARK_PYTHON\"] = (\n",
    "        f\"/home/circleci/miniconda/envs/{default_env}/bin/python\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3596c7",
   "metadata": {},
   "source": [
    "# Time Series Sub-Module\n",
    "\n",
    "This tutorial walks through some of the basics of the time series submodule, which\n",
    "leverages Spark's higher-order-functions to perform calculations, converting the\n",
    "problem from tall format to wide format.\n",
    "\n",
    "See [the tall vs wide experiment](./10_timeseries_experiment.md) for an analysis on\n",
    "performance of slicing (window function equivalent) at scale.\n",
    "\n",
    "\n",
    "A few other points:\n",
    "\n",
    "* If you are not familar with higher-order-functions, you can follow Databrick's tutorial\n",
    "[here](https://docs.databricks.com/delta/data-transformation/higher-order-lambda-functions.html).\n",
    "* In Spark 3.1, we can write actual python functions rather than SQL strings. See\n",
    "[here](https://towardsdatascience.com/higher-order-functions-with-spark-3-1-7c6cf591beaa).\n",
    "* Here is an interesting article that benchmarks different approaches and compares the performance\n",
    "of higher-order-functions: [article](https://towardsdatascience.com/performance-in-apache-spark-benchmark-9-different-techniques-955d3cc93266).\n",
    "\n",
    "## Motivation\n",
    "\n",
    "When it comes to analysing timeseries data or any data that requires ordering such as\n",
    "sensor data, it is believed that tall format might not be the most ideal data structure\n",
    "for such problems. This is because one cannot assume the data is sorted without first\n",
    "explicitly ordering the data, especially when it comes to spark where the underlying\n",
    "data is stored in various files. Example below when we read data from file:\n",
    "\n",
    "```\n",
    "# data could be this\n",
    "| hcp_id | time_index | value |\n",
    "|--------|------------|-------|\n",
    "| h1     | 1          | 10    |\n",
    "| h1     | 2          | 3     |\n",
    "| h1     | 3          | 4     |\n",
    "| h1     | 4          | 2     |\n",
    "| h1     | 5          | 7     |\n",
    "| h1     | 6          | 2     |\n",
    "| h2     | 1          | 4     |\n",
    "| h2     | 2          | 7     |\n",
    "| h2     | 3          | 2     |\n",
    "| h2     | 4          | 8     |\n",
    "| h2     | 5          | 2     |\n",
    "| h2     | 6          | 9     |\n",
    "\n",
    "# or this\n",
    "| hcp_id | time_index | value |\n",
    "|--------|------------|-------|\n",
    "| h1     | 1          | 10    |\n",
    "| h1     | 6          | 2     |\n",
    "| h1     | 3          | 4     |\n",
    "| h1     | 4          | 2     |\n",
    "| h1     | 5          | 7     |\n",
    "| h1     | 2          | 3     |\n",
    "| h2     | 6          | 9     |\n",
    "| h2     | 2          | 7     |\n",
    "| h2     | 1          | 4     |\n",
    "| h2     | 4          | 8     |\n",
    "| h2     | 5          | 2     |\n",
    "| h2     | 3          | 2     |\n",
    "```\n",
    "\n",
    "Additionally, in spark, even if you sort the data, writing to file and re-reading does not preserve\n",
    "this row ordering. One has to explicitly re-order the data before performing any operation\n",
    "in order to guarantee that order is respected. When chaining multiple operations together,\n",
    "each operation still needs to explicitly order and cannot assume the data is ordered from\n",
    "the operation prior. This leads to inefficiencies in a big data setting as the data\n",
    "needs to be constantly shuffled and explicitly ordered each time a calculation is performed.\n",
    "Note that shuffling data is arguably one of the most expensive operations in spark.\n",
    "\n",
    "In the timeseries module, the main data structure is leveraging arrays, where order\n",
    "between rows are always preserved. The idea is to utilize the ordering to make calculations\n",
    "(that require ordering) more efficient because there is no need to explicitly order the\n",
    "data each time, which in spark may trigger a shuffle:\n",
    "\n",
    "```\n",
    "| hcp_id | time_index_array | value_array    |\n",
    "|--------|------------------|----------------|\n",
    "| h1     | [1,2,3,4,5,6]    | [10,3,4,2,7,2] |\n",
    "| h2     | [1,2,3,4,5,6]    | [4,7,2,8,2,9]  |\n",
    "```\n",
    "\n",
    "The array structure also has another benefit, which is to allow more complicated calculations\n",
    "to be performed on ordered data, as one can easily use `udf`s, or distribute any `sklearn`\n",
    "function over a cluster, or leverage spark's higher-order-function capabilities. Stateful calculations\n",
    "are easy to write when one's input is a list, as one can iterate through the list in a\n",
    "normal for-loop and store a state that is modified along the way. While window functions\n",
    "let's you perform a few basic stateful calculations, they fall short when something more\n",
    "complicated is needed, for example, performing exponential smoothing or discovering seasonality.\n",
    "\n",
    "To summarize, the purpose of this submodule is to:\n",
    "\n",
    "* leverage array data structures for performance gains on ordered data\n",
    "* enable more complicated calculations when required\n",
    "\n",
    "## Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85891d1",
   "metadata": {
    "tags": [
     "hide-input",
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.config(\"spark.ui.showConsoleProgress\", False).getOrCreate()\n",
    "import datetime\n",
    "\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "from pyspark.sql.types import (\n",
    "    IntegerType,\n",
    "    StringType,\n",
    "    StructField,\n",
    "    StructType,\n",
    "    TimestampType,\n",
    ")\n",
    "\n",
    "from feature_generation.v1.core.timeseries.array_collect import (\n",
    "    collect_array_then_interpolate,\n",
    ")\n",
    "from feature_generation.v1.core.timeseries.array_transform import array_smooth_ts_values\n",
    "\n",
    "\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"element_id\", StringType(), True),\n",
    "        StructField(\"reading_ts\", TimestampType(), True),\n",
    "        StructField(\"reading_val\", IntegerType(), True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "data = [\n",
    "    (\"line1\", datetime.datetime(1970, 1, 1, 5, 0), 1),\n",
    "    (\"line1\", datetime.datetime(1970, 1, 1, 5, 2), 2),\n",
    "    (\"line1\", datetime.datetime(1970, 1, 1, 5, 6), 3),\n",
    "    (\"line1\", datetime.datetime(1970, 1, 1, 5, 7), 4),\n",
    "    (\"line1\", datetime.datetime(1970, 1, 1, 5, 8), 5),\n",
    "    (\"line2\", datetime.datetime(1970, 1, 1, 7, 1), 3),\n",
    "    (\"line2\", datetime.datetime(1970, 1, 1, 7, 5), 1),\n",
    "    (\n",
    "        \"line2\",\n",
    "        datetime.datetime(1970, 1, 1, 7, 10),\n",
    "        8,\n",
    "    ),\n",
    "    (\n",
    "        \"line2\",\n",
    "        datetime.datetime(1970, 1, 1, 7, 11),\n",
    "        6,\n",
    "    ),\n",
    "    (\n",
    "        \"line2\",\n",
    "        datetime.datetime(1970, 1, 1, 7, 12),\n",
    "        7,\n",
    "    ),\n",
    "]\n",
    "mock_datetime_df = spark.createDataFrame(data, schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc529448",
   "metadata": {},
   "source": [
    "For this demonstration, we will be using the following dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d163e219",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "mock_datetime_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0782cd8c",
   "metadata": {},
   "source": [
    "When dealing with time, we recommend adding a time index. In this case, we will be\n",
    "adding a minute index. But depending on the problem, you might want to add an hour index,\n",
    "day index, second index and so on. There are several time index creation functions that\n",
    "are already available, so we simply import the `minute_index` function and use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d99e171",
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_generation.v1.core.timeseries.datetime import minute_index\n",
    "\n",
    "mock_datetime_df = mock_datetime_df.withColumn(\n",
    "    \"minute_index\", minute_index(\"reading_ts\")\n",
    ")\n",
    "mock_datetime_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78be3fb",
   "metadata": {},
   "source": [
    "Next, we need to collect the values in tall format, and convert them to arrays (wide\n",
    "format):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d5ff60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_generation.v1.core.timeseries.array_collect import (\n",
    "    collect_array_then_interpolate,\n",
    ")\n",
    "from feature_generation.v1.core.timeseries.array_transform import (\n",
    "    interpolate_constant,\n",
    "    scipy_interpolate,\n",
    ")\n",
    "\n",
    "collected_array_df = collect_array_then_interpolate(\n",
    "    df=mock_datetime_df,\n",
    "    order=\"minute_index\",\n",
    "    values=[\"reading_val\"],\n",
    "    groupby=\"element_id\",\n",
    "    spine=\"spine\",\n",
    "    interpolate_func=interpolate_constant,\n",
    ")\n",
    "\n",
    "collected_array_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5006fd",
   "metadata": {},
   "source": [
    "The spine for each row is generated from the min and max values in the original array.\n",
    "Having a complete timeseries without gaps is crucial when it comes to analysing timeseries\n",
    "data. This means we can apply most calculations without worry of any gaps, and allows\n",
    "us to easily downsample when we need to.\n",
    "\n",
    "\n",
    "The next step is to interpolate the values and fill in the missing values from the spine.\n",
    "In this case, we simply want to forward fill:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5691f41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_generation.v1.core.timeseries.array_transform import scipy_interpolate\n",
    "\n",
    "collected_array_df = collected_array_df.withColumn(\n",
    "    \"interpolated\",\n",
    "    scipy_interpolate(\n",
    "        spine_index=\"spine\",\n",
    "        time_index=\"minute_index\",\n",
    "        value=\"reading_val\",\n",
    "        kind=\"previous\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "collected_array_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cc138d",
   "metadata": {},
   "source": [
    "Note that the interpolate is a UDF and uses `scipy`'s [interp1d](https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.interp1d.html)\n",
    "function. This may not be the best implementation, but it certainly prevents a lot of\n",
    "re-inventing the wheel for now.\n",
    "\n",
    "Once we have our complete signal, we can start to extract information out of it.\n",
    "\n",
    "There are 2 main types of functions within the time series module:\n",
    "\n",
    "* `array_aggregate` - receives a vector and returns a scalar\n",
    "* `array_transform` - receives a vector and returns another vector\n",
    "\n",
    "\n",
    "Here is an example of how to extract the min and max within the arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc6f0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_generation.v1.core.timeseries.array_aggregate import (\n",
    "    array_max,\n",
    "    array_min,\n",
    ")\n",
    "\n",
    "features = collected_array_df.withColumn(\n",
    "    \"feature_array_max\", array_max(value=\"interpolated\")\n",
    ").withColumn(\"feature_array_min\", array_min(value=\"interpolated\"))\n",
    "\n",
    "features.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c261c5b",
   "metadata": {},
   "source": [
    "Or if we want to smooth the values (the equivalent of taking the average within a\n",
    "window):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261262d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_generation.v1.core.timeseries.array_transform import array_smooth_ts_values\n",
    "\n",
    "\n",
    "features = collected_array_df.withColumn(\n",
    "    \"smoothed\", array_smooth_ts_values(value=\"interpolated\", length=3)\n",
    ")\n",
    "\n",
    "features.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1853e3eb",
   "metadata": {},
   "source": [
    "### Aggregate over slice\n",
    "Aggregate over slice method uses wide format instead of traditional long format (group by, agg over window etc.).\n",
    "\n",
    "Input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be99bbd8",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from pyspark import Row\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "import datetime\n",
    "\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "\n",
    "df_spine = spark.createDataFrame(\n",
    "    [\n",
    "        Row(\n",
    "            npi_id=\"h1\",\n",
    "            time_index=1,\n",
    "        ),\n",
    "        Row(\n",
    "            npi_id=\"h1\",\n",
    "            time_index=2,\n",
    "        ),\n",
    "        Row(\n",
    "            npi_id=\"h1\",\n",
    "            time_index=3,\n",
    "        ),\n",
    "        Row(\n",
    "            npi_id=\"h1\",\n",
    "            time_index=4,\n",
    "        ),\n",
    "        Row(\n",
    "            npi_id=\"h1\",\n",
    "            time_index=5,\n",
    "        ),\n",
    "        Row(\n",
    "            npi_id=\"h1\",\n",
    "            time_index=6,\n",
    "        ),\n",
    "        Row(\n",
    "            npi_id=\"h1\",\n",
    "            time_index=7,\n",
    "        ),\n",
    "        Row(\n",
    "            npi_id=\"h1\",\n",
    "            time_index=8,\n",
    "        ),\n",
    "        Row(\n",
    "            npi_id=\"h1\",\n",
    "            time_index=9,\n",
    "        ),\n",
    "        Row(\n",
    "            npi_id=\"h1\",\n",
    "            time_index=10,\n",
    "        ),\n",
    "        Row(\n",
    "            npi_id=\"h1\",\n",
    "            time_index=11,\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "df_input_slice = spark.createDataFrame(\n",
    "    [\n",
    "        Row(\n",
    "            npi_id=\"h1\",\n",
    "            time_index=1,\n",
    "            value=0,\n",
    "        ),\n",
    "        Row(\n",
    "            npi_id=\"h1\",\n",
    "            time_index=2,\n",
    "            value=1,\n",
    "        ),\n",
    "        Row(\n",
    "            npi_id=\"h1\",\n",
    "            time_index=3,\n",
    "            value=2,\n",
    "        ),\n",
    "        Row(\n",
    "            npi_id=\"h1\",\n",
    "            time_index=4,\n",
    "            value=3,\n",
    "        ),\n",
    "        Row(\n",
    "            npi_id=\"h1\",\n",
    "            time_index=7,\n",
    "            value=6,\n",
    "        ),\n",
    "        Row(\n",
    "            npi_id=\"h1\",\n",
    "            time_index=8,\n",
    "            value=7,\n",
    "        ),\n",
    "        Row(\n",
    "            npi_id=\"h1\",\n",
    "            time_index=10,\n",
    "            value=9,\n",
    "        ),\n",
    "        Row(\n",
    "            npi_id=\"h1\",\n",
    "            time_index=11,\n",
    "            value=10,\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "print(\"Input dataframe\")\n",
    "df_input_slice.show(truncate=False)\n",
    "\n",
    "print(\"Spine dataframe\")\n",
    "df_spine.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca7cea4",
   "metadata": {},
   "source": [
    "Here, as a prerequisite, all the input values and the corresponding time component should be collected in lists in separate columns.\n",
    "\n",
    "There are a few ways to do that:\n",
    "\n",
    "1. Aggregation over window.\n",
    "Pre-requisite for this method is to join the dataframe with spine and fill nulls wherever the values are missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd745ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "\n",
    "w = (\n",
    "    Window.partitionBy(\"npi_id\")\n",
    "    .orderBy(\"time_index\")\n",
    "    .rangeBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n",
    ")\n",
    "df_input_slice_tall = df_spine.join(\n",
    "    df_input_slice, [\"npi_id\", \"time_index\"], \"left\"\n",
    ").fillna(0, subset=[\"value\"])\n",
    "\n",
    "df_collect_list = df_input_slice_tall.withColumn(\n",
    "    \"time_list\", f.collect_list(\"time_index\").over(w)\n",
    ").withColumn(\"value_list\", f.collect_list(\"value\").over(w))\n",
    "\n",
    "df_collect_list.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367f54a7",
   "metadata": {},
   "source": [
    "Downside to this method is that the spark might start getting out of memory if working on really large dataset\n",
    "because the data is being replicated for every row.\n",
    "\n",
    "2. Use `groupby` and `aggregate` using `sorted_collect_set` - all of which are performed under\n",
    "`collect_array_then_interpolate` (see source code if curious).\n",
    "\n",
    "This is currently the recommended way as this is shown to be more performant based on the testing performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3319e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from feature_generation.v1.core.timeseries.array_transform import (\n",
    "    interpolate_constant,\n",
    ")\n",
    "\n",
    "\n",
    "w = Window.partitionBy(\"npi_id\")\n",
    "list_of_tags = [\"value\"]\n",
    "\n",
    "df_collect_list_grp = collect_array_then_interpolate(\n",
    "    df=df_input_slice,\n",
    "    order=\"time_index\",\n",
    "    values=list_of_tags,\n",
    "    groupby=\"npi_id\",\n",
    "    spine=\"spine_index\",\n",
    "    interpolate_func=interpolate_constant,\n",
    ")\n",
    "\n",
    "# Join with spine to get to the spine granularity.\n",
    "df_collect_list_2 = df_spine.join(\n",
    "    df_collect_list_grp.drop(\"time_index\"), \"npi_id\", \"left\"\n",
    ").select(\"npi_id\", \"time_index\", \"spine_index\", \"value_array_padded\")\n",
    "\n",
    "df_collect_list_2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36089a4f",
   "metadata": {},
   "source": [
    "For each row, time index acts as the anchor and its corresponding index is searched in time_index_array. Based on this anchor index and the window lower bound and upper bound, the value array is sliced and required aggregation is applied on the sliced array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e7a899",
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_generation.v1.core.timeseries.array_aggregate import (\n",
    "    aggregate_over_slice,\n",
    "    array_sum,\n",
    ")\n",
    "\n",
    "df_result = df_collect_list.select(\n",
    "    \"*\",\n",
    "    aggregate_over_slice(\n",
    "        input_col=\"value_list\",\n",
    "        lower_bound=-1,\n",
    "        upper_bound=3,\n",
    "        anchor=\"time_index\",\n",
    "        anchor_array=\"time_list\",\n",
    "        func=array_sum,\n",
    "        alias=\"sum_val\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "df_result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35898193",
   "metadata": {},
   "source": [
    "## Summary of Functions\n",
    "\n",
    "Below is a summary of the available functions within the sub-module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc040db9",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from types import FunctionType\n",
    "from feature_generation.v1.core import timeseries\n",
    "import pkgutil\n",
    "\n",
    "\n",
    "pkgname = timeseries.__name__\n",
    "pkgpath = timeseries.__path__[0]\n",
    "found_packages = list(pkgutil.iter_modules([pkgpath], prefix=\"{}.\".format(pkgname)))\n",
    "importer = found_packages[0][0]\n",
    "sub_packages = [x.split(\".\")[-1] for _, x, _ in found_packages]\n",
    "\n",
    "func_row = []\n",
    "for idx, _name in enumerate(sub_packages):\n",
    "    module_spec = importer.find_spec(found_packages[idx][1])\n",
    "    module = module_spec.loader.load_module(found_packages[idx][1])\n",
    "\n",
    "    list_of_tags = [\n",
    "        x\n",
    "        for x in dir(module)\n",
    "        if isinstance(getattr(module, x), FunctionType) and not x.startswith(\"_\")\n",
    "    ]\n",
    "\n",
    "    from tabulate import tabulate\n",
    "\n",
    "    for x in list_of_tags:\n",
    "        x_doc = getattr(module, x).__doc__.split(\"\\n\")[0]\n",
    "        func_row.append(\n",
    "            {\"sub_module\": module.__name__, \"function\": x, \"description\": x_doc}\n",
    "        )\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "table = pd.DataFrame(func_row)\n",
    "print(tabulate(table, headers=table.columns, tablefmt=\"psql\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
