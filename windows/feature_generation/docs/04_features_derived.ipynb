{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47de077b",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"../\")\n",
    "sys.path.insert(0, \"packages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84b3dbe",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if os.environ.get(\"CIRCLECI\"):\n",
    "    default_env = os.environ.get(\"CONDA_DEFAULT_ENV\")\n",
    "    os.environ[\"PYSPARK_DRIVER_PYTHON\"] = (\n",
    "        f\"/home/circleci/miniconda/envs/{default_env}/bin/python\"\n",
    "    )\n",
    "    os.environ[\"PYSPARK_PYTHON\"] = (\n",
    "        f\"/home/circleci/miniconda/envs/{default_env}/bin/python\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f0ed45",
   "metadata": {},
   "source": [
    "# Derived Features\n",
    "\n",
    "This document outlines the functions available to create derived features using\n",
    "examples.\n",
    "\n",
    "We will be using the following dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7606c06b",
   "metadata": {
    "tags": [
     "hide-input",
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import (\n",
    "    ArrayType,\n",
    "    DateType,\n",
    "    IntegerType,\n",
    "    StringType,\n",
    "    StructField,\n",
    "    StructType,\n",
    ")\n",
    "\n",
    "spark = SparkSession.builder.config(\"spark.ui.showConsoleProgress\", False).getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"OFF\")\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"date\", DateType(), True),\n",
    "        StructField(\"date_index\", IntegerType(), True),\n",
    "        StructField(\"x_flag\", IntegerType(), True),\n",
    "        StructField(\"y_flag\", IntegerType(), True),\n",
    "    ]\n",
    ")\n",
    "data = [\n",
    "    (\"Gendry\", pd.Timestamp(\"2012-05-01\").date(), 15461, 1, 0),\n",
    "    (\"Gendry\", pd.Timestamp(\"2012-05-02\").date(), 15462, 0, None),\n",
    "    (\"Gendry\", pd.Timestamp(\"2012-05-03\").date(), 15463, 1, None),\n",
    "    (\"Gendry\", pd.Timestamp(\"2012-05-04\").date(), 15464, 0, 1),\n",
    "    (\"Gendry\", pd.Timestamp(\"2012-05-05\").date(), 15465, 1, None),\n",
    "    (\"Arya\", pd.Timestamp(\"2012-05-06\").date(), 15466, 1, 0),\n",
    "    (\"Arya\", pd.Timestamp(\"2012-05-07\").date(), 15467, 0, None),\n",
    "    (\"Arya\", pd.Timestamp(\"2012-05-08\").date(), 15468, 0, 1),\n",
    "    (\"Arya\", pd.Timestamp(\"2012-05-09\").date(), 15469, 1, None),\n",
    "    (\"Arya\", pd.Timestamp(\"2012-05-10\").date(), 15470, 1, 2),\n",
    "    (\"Cersei\", pd.Timestamp(\"2012-05-10\").date(), 15470, 0, 0),\n",
    "    (\"Cersei\", pd.Timestamp(\"2012-05-11\").date(), 15471, 1, None),\n",
    "    (\"Cersei\", pd.Timestamp(\"2012-05-12\").date(), 15472, 1, None),\n",
    "    (\"Cersei\", pd.Timestamp(\"2012-05-13\").date(), 15473, 1, None),\n",
    "    (\"Cersei\", pd.Timestamp(\"2012-05-15\").date(), 15475, 0, 1),\n",
    "]\n",
    "df_window = spark.createDataFrame(data, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7186b38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_window.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54efa582",
   "metadata": {},
   "source": [
    "## Windows\n",
    "\n",
    "This util provides a function that takes a spark dataframe and creates window\n",
    "features based on a dictionary or config. It provides the ability to create a single window\n",
    "feature per entry or can create multiple window features given a grid of columns,\n",
    "aggregation functions, windows, and ranges. This can be used as a part of the\n",
    "`create_column_from_config` function.\n",
    "\n",
    "### `generate_windows_spec`:\n",
    "Creates a list of `WindowSpec` objects, based on the config passed.\n",
    "This function can be used when working with custom functions to remove\n",
    "repetitive window creations.\n",
    "\n",
    "The output of this function would essentialy be used by a function which\n",
    "accepts window specs (`generate_window_grid` , `window_column`) or you can\n",
    "aggregate over this window spec while creating new columns.\n",
    "\n",
    "Code example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3a3461",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as f\n",
    "from feature_generation.v1.core.features.windows import generate_windows_spec\n",
    "\n",
    "windows_spec = generate_windows_spec(\n",
    "    partition_by=\"name\", order_by=\"date_index\", range_between=[-5, -1]\n",
    ")\n",
    "\n",
    "df_windows_spec = df_window.withColumn(\n",
    "    \"x_flag_min_last_5d\", f.min(\"x_flag\").over(windows_spec[0])\n",
    ")\n",
    "\n",
    "df_windows_spec.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42243d0",
   "metadata": {},
   "source": [
    "Note that the `generate_windows_spec` returns a list of windows spec.\n",
    "\n",
    "Which is equivalent to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eee6ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "\n",
    "df_windows_spec = df_window.withColumn(\n",
    "    \"x_flag_min_last_5d\",\n",
    "    f.min(\"x_flag\").over(\n",
    "        Window.partitionBy(\"name\").orderBy(\"date_index\").rangeBetween(-5, -1)\n",
    "    ),\n",
    ")\n",
    "\n",
    "df_windows_spec.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e509a8",
   "metadata": {},
   "source": [
    "### `window_column`:\n",
    "Creates a new column given the window configuration.\n",
    "\n",
    "\n",
    "Core example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b577b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql import Window\n",
    "from feature_generation.v1.core.features.create_column import create_columns_from_config\n",
    "from feature_generation.v1.core.features.windows import window_column\n",
    "\n",
    "windows_col_code_config = [\n",
    "    window_column(\n",
    "        \"x_flag_sum_last_3d\",\n",
    "        f.sum(\"x_flag\"),\n",
    "        Window.partitionBy(\"name\").orderBy(\"date_index\").rangeBetween(-3, -1),\n",
    "    ),\n",
    "    window_column(\n",
    "        [\"x_flag_mean_last_2d\", \"x_flag_mean_last_3d\"],\n",
    "        f.mean(\"x_flag\"),\n",
    "        [\n",
    "            Window.partitionBy(\"name\").orderBy(\"date_index\").rowsBetween(-2, -2),\n",
    "            Window.partitionBy(\"name\").orderBy(\"date_index\").rowsBetween(-3, -3),\n",
    "        ],\n",
    "    ),\n",
    "]\n",
    "\n",
    "df_windows_col_code = create_columns_from_config(df_window, windows_col_code_config)\n",
    "\n",
    "df_windows_col_code.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5707e2",
   "metadata": {},
   "source": [
    "Node example below (we will need to use the `generate_window_spec` function if calling\n",
    "from parameters):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97aa38b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_generation.v1.nodes.features.create_column import (\n",
    "    create_columns_from_config,\n",
    ")\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql import Window\n",
    "\n",
    "windows_col_config = [\n",
    "    {\n",
    "        \"object\": \"feature_generation.v1.core.features.windows.window_column\",\n",
    "        \"outputs\": \"x_flag_sum_last_3d\",\n",
    "        \"input\": {\"object\": \"pyspark.sql.functions.sum\", \"col\": \"x_flag\"},\n",
    "        \"windows_spec\": {\n",
    "            \"object\": \"feature_generation.v1.core.features.windows.generate_windows_spec\",\n",
    "            \"partition_by\": \"name\",\n",
    "            \"order_by\": \"date_index\",\n",
    "            \"range_between\": [-3, -1],\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"object\": \"feature_generation.v1.core.features.windows.window_column\",\n",
    "        \"outputs\": [\"x_flag_mean_last_2d\", \"x_flag_mean_last_3d\"],\n",
    "        \"input\": {\"object\": \"pyspark.sql.functions.mean\", \"col\": \"x_flag\"},\n",
    "        \"windows_spec\": {\n",
    "            \"object\": \"feature_generation.v1.core.features.windows.generate_windows_spec\",\n",
    "            \"partition_by\": \"name\",\n",
    "            \"order_by\": \"date_index\",\n",
    "            \"rows_between\": [[-2, -2], [-3, -3]],\n",
    "        },\n",
    "    },\n",
    "]\n",
    "\n",
    "df_windows_col = create_columns_from_config(df_window, windows_col_config)\n",
    "\n",
    "df_windows_col.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441d9f23",
   "metadata": {},
   "source": [
    "### `generate_window_grid`:\n",
    "Generates window columns given a configuration grid. Grid should contain columns,\n",
    "aggregation functions, windows, and ranges. Note that the user must ensure\n",
    "compatibility of all combinations within the grid.\n",
    "\n",
    "\n",
    "Core example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd18a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as f\n",
    "from feature_generation.v1.core.features.create_column import create_columns_from_config\n",
    "from feature_generation.v1.core.features.windows import generate_window_grid\n",
    "\n",
    "windows_grid_code_config = [\n",
    "    generate_window_grid(\n",
    "        inputs=[\"x_flag\", \"y_flag\"],\n",
    "        funcs=[f.max, f.sum],\n",
    "        windows=[\n",
    "            {\"partition_by\": [\"name\"], \"order_by\": [\"date_index\"], \"descending\": False},\n",
    "            {\"partition_by\": [\"name\"], \"order_by\": [\"date_index\"], \"descending\": True},\n",
    "        ],\n",
    "        ranges_between=[\n",
    "            [-1, -1],\n",
    "            [0, 0],\n",
    "            [1, 1],\n",
    "            [-1, \"UNBOUNDED FOLLOWING\"],\n",
    "            [\"unbounded preceding\", 0],\n",
    "        ],\n",
    "        negative_term=\"past\",\n",
    "        positive_term=\"next\",\n",
    "        suffix=\"d\",\n",
    "    )\n",
    "]\n",
    "\n",
    "df_windows_grid_code = create_columns_from_config(df_window, windows_grid_code_config)\n",
    "\n",
    "df_windows_grid_code.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709fef8c",
   "metadata": {},
   "source": [
    "Node example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966b1bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_generation.v1.nodes.features.create_column import (\n",
    "    create_columns_from_config,\n",
    ")\n",
    "\n",
    "window_grid_config = [\n",
    "    {\n",
    "        \"object\": \"feature_generation.v1.core.features.windows.generate_window_grid\",\n",
    "        \"inputs\": [\"x_flag\", \"y_flag\"],\n",
    "        \"funcs\": [\n",
    "            {\"object\": \"pyspark.sql.functions.sum\"},\n",
    "            {\"object\": \"pyspark.sql.functions.max\"},\n",
    "        ],\n",
    "        \"windows\": [\n",
    "            {\"partition_by\": [\"name\"], \"order_by\": [\"date_index\"], \"descending\": False},\n",
    "            {\"partition_by\": [\"name\"], \"order_by\": [\"date_index\"], \"descending\": True},\n",
    "        ],\n",
    "        \"ranges_between\": [\n",
    "            [-1, -1],\n",
    "            [0, 0],\n",
    "            [1, 1],\n",
    "            [-1, \"UNBOUNDED FOLLOWING\"],\n",
    "            [\"unbounded preceding\", 0],\n",
    "        ],\n",
    "        \"negative_term\": \"past\",\n",
    "        \"positive_term\": \"next\",\n",
    "        \"suffix\": \"d\",\n",
    "    },\n",
    "]\n",
    "\n",
    "df_windows_grid = create_columns_from_config(df_window, window_grid_config)\n",
    "\n",
    "df_windows_grid.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c657ee",
   "metadata": {},
   "source": [
    "### `generate_distinct_element_window_grid`:\n",
    "Generates window columns with distinct elements in given a grid. Grid should contain\n",
    "columns, windows, and ranges. Note that the input columnn should only have array of\n",
    "elements we want to calculate over a grid.\n",
    "\n",
    "We will be using the following dataframe for generate_distinct_element_window_grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55eedef",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import (\n",
    "    ArrayType,\n",
    "    DateType,\n",
    "    IntegerType,\n",
    "    StringType,\n",
    "    StructField,\n",
    "    StructType,\n",
    ")\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"date\", DateType(), True),\n",
    "        StructField(\"date_index\", IntegerType(), True),\n",
    "        StructField(\"x_flag\", IntegerType(), True),\n",
    "        StructField(\"y_flag\", IntegerType(), True),\n",
    "        StructField(\"array_col\", ArrayType(StringType()), True),\n",
    "    ]\n",
    ")\n",
    "data = [\n",
    "    (\n",
    "        \"Gendry\",\n",
    "        pd.Timestamp(\"2012-05-01\").date(),\n",
    "        15461,\n",
    "        1,\n",
    "        0,\n",
    "        ([\"a\", \"b\", \"b\", \"a\", \"a\"]),\n",
    "    ),\n",
    "    (\n",
    "        \"Gendry\",\n",
    "        pd.Timestamp(\"2012-05-02\").date(),\n",
    "        15462,\n",
    "        0,\n",
    "        None,\n",
    "        ([\"a\", \"b\", \"b\", \"a\", \"a\"]),\n",
    "    ),\n",
    "    (\n",
    "        \"Gendry\",\n",
    "        pd.Timestamp(\"2012-05-03\").date(),\n",
    "        15463,\n",
    "        1,\n",
    "        None,\n",
    "        ([\"b\", \"c\", \"a\", \"a\"]),\n",
    "    ),\n",
    "    (\n",
    "        \"Gendry\",\n",
    "        pd.Timestamp(\"2012-05-04\").date(),\n",
    "        15464,\n",
    "        0,\n",
    "        1,\n",
    "        ([\"b\", \"a\", \"a\"]),\n",
    "    ),\n",
    "    (\n",
    "        \"Gendry\",\n",
    "        pd.Timestamp(\"2012-05-05\").date(),\n",
    "        15465,\n",
    "        1,\n",
    "        None,\n",
    "        ([\"a\", \"a\"]),\n",
    "    ),\n",
    "    (\n",
    "        \"Arya\",\n",
    "        pd.Timestamp(\"2012-05-06\").date(),\n",
    "        15466,\n",
    "        1,\n",
    "        0,\n",
    "        ([\"b\", \"a\", \"a\", \"a\", \"b\"]),\n",
    "    ),\n",
    "    (\n",
    "        \"Arya\",\n",
    "        pd.Timestamp(\"2012-05-07\").date(),\n",
    "        15467,\n",
    "        0,\n",
    "        None,\n",
    "        ([\"b\", \"a\", \"a\", \"a\", \"b\"]),\n",
    "    ),\n",
    "    (\n",
    "        \"Arya\",\n",
    "        pd.Timestamp(\"2012-05-08\").date(),\n",
    "        15468,\n",
    "        0,\n",
    "        1,\n",
    "        ([\"a\", \"a\", \"c\", \"b\"]),\n",
    "    ),\n",
    "    (\n",
    "        \"Arya\",\n",
    "        pd.Timestamp(\"2012-05-09\").date(),\n",
    "        15469,\n",
    "        1,\n",
    "        None,\n",
    "        ([\"a\", \"a\", \"b\"]),\n",
    "    ),\n",
    "    (\n",
    "        \"Arya\",\n",
    "        pd.Timestamp(\"2012-05-10\").date(),\n",
    "        15470,\n",
    "        1,\n",
    "        2,\n",
    "        ([\"b\"]),\n",
    "    ),\n",
    "    (\n",
    "        \"Cersei\",\n",
    "        pd.Timestamp(\"2012-05-10\").date(),\n",
    "        15470,\n",
    "        0,\n",
    "        0,\n",
    "        ([\"a\", \"b\", \"c\", \"d\", \"a\"]),\n",
    "    ),\n",
    "    (\n",
    "        \"Cersei\",\n",
    "        pd.Timestamp(\"2012-05-11\").date(),\n",
    "        15471,\n",
    "        1,\n",
    "        None,\n",
    "        ([\"a\", \"b\", \"a\", \"b\", \"a\"]),\n",
    "    ),\n",
    "    (\n",
    "        \"Cersei\",\n",
    "        pd.Timestamp(\"2012-05-12\").date(),\n",
    "        15472,\n",
    "        1,\n",
    "        None,\n",
    "        ([\"b\", \"c\", \"b\", \"a\"]),\n",
    "    ),\n",
    "    (\n",
    "        \"Cersei\",\n",
    "        pd.Timestamp(\"2012-05-13\").date(),\n",
    "        15473,\n",
    "        1,\n",
    "        None,\n",
    "        ([\"a\", \"b\", \"a\"]),\n",
    "    ),\n",
    "    (\n",
    "        \"Cersei\",\n",
    "        pd.Timestamp(\"2012-05-15\").date(),\n",
    "        15475,\n",
    "        0,\n",
    "        1,\n",
    "        ([\"b\", \"a\"]),\n",
    "    ),\n",
    "]\n",
    "df_array = spark.createDataFrame(data, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f04ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_array.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ad6ace",
   "metadata": {},
   "source": [
    "Core example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940d2ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_generation.v1.nodes.features.create_column import (\n",
    "    create_columns_from_config,\n",
    ")\n",
    "from pyspark.sql import functions as f\n",
    "from feature_generation.v1.core.features.create_column import create_columns_from_config\n",
    "from feature_generation.v1.core.features.windows import (\n",
    "    generate_distinct_element_window_grid,\n",
    ")\n",
    "\n",
    "distinct_windows_grid_code_config = [\n",
    "    generate_distinct_element_window_grid(\n",
    "        inputs=[\n",
    "            \"array_col\",\n",
    "        ],\n",
    "        windows=[\n",
    "            {\"partition_by\": [\"name\"], \"order_by\": [\"date_index\"], \"descending\": False},\n",
    "        ],\n",
    "        ranges_between=[\n",
    "            [-1, -1],\n",
    "        ],\n",
    "        negative_term=\"past\",\n",
    "        positive_term=\"next\",\n",
    "        suffix=\"d\",\n",
    "    )\n",
    "]\n",
    "\n",
    "df_distinct_windows_grid_code = create_columns_from_config(\n",
    "    df_array, distinct_windows_grid_code_config\n",
    ")\n",
    "df_distinct_windows_grid_code.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103237c5",
   "metadata": {},
   "source": [
    "Node example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5010ff96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_generation.v1.nodes.features.create_column import (\n",
    "    create_columns_from_config,\n",
    ")\n",
    "\n",
    "distinct_window_grid_config = [\n",
    "    {\n",
    "        \"object\": \"feature_generation.v1.core.features.windows.generate_distinct_element_window_grid\",\n",
    "        \"inputs\": [\n",
    "            \"array_col\",\n",
    "        ],\n",
    "        \"windows\": [\n",
    "            {\"partition_by\": [\"name\"], \"order_by\": [\"date_index\"], \"descending\": False},\n",
    "        ],\n",
    "        \"ranges_between\": [\n",
    "            [-1, -1],\n",
    "        ],\n",
    "        \"negative_term\": \"past\",\n",
    "        \"positive_term\": \"next\",\n",
    "        \"suffix\": \"d\",\n",
    "    },\n",
    "]\n",
    "\n",
    "df_distinct_windows_grid = create_columns_from_config(\n",
    "    df_array, distinct_window_grid_config\n",
    ")\n",
    "\n",
    "df_distinct_windows_grid.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651d3a01",
   "metadata": {},
   "source": [
    "### `generate_window_ratio`:\n",
    "Generates ratio of window columns given a grid. Grid should contain columns,\n",
    "aggregation functions, windows, and ranges. Note that the user must ensure compatibility\n",
    " of all combinations within the grid.\n",
    "\n",
    "Core example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428deea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as f\n",
    "from feature_generation.v1.core.features.create_column import create_columns_from_config\n",
    "from feature_generation.v1.core.features.windows import generate_window_ratio\n",
    "\n",
    "df_window = df_window.withColumn(\"total_flag\", f.col(\"x_flag\") + f.col(\"y_flag\"))\n",
    "\n",
    "windows_ratio_code_config = [\n",
    "    generate_window_ratio(\n",
    "        inputs={\"x_flag\": \"total_flag\", \"y_flag\": \"total_flag\"},\n",
    "        funcs=[f.max, f.sum],\n",
    "        windows=[\n",
    "            {\"partition_by\": [\"name\"], \"order_by\": [\"date_index\"], \"descending\": False},\n",
    "        ],\n",
    "        ranges_between=[\n",
    "            [-1, -1],\n",
    "            [0, 0],\n",
    "            [1, 1],\n",
    "        ],\n",
    "        negative_term=\"past\",\n",
    "        positive_term=\"next\",\n",
    "        suffix=\"d\",\n",
    "    )\n",
    "]\n",
    "\n",
    "df_windows_ratio_code = create_columns_from_config(df_window, windows_ratio_code_config)\n",
    "df_windows_ratio_code.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4870dc3b",
   "metadata": {},
   "source": [
    "Node example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e72d229",
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_generation.v1.nodes.features.create_column import (\n",
    "    create_columns_from_config,\n",
    ")\n",
    "\n",
    "df_window = df_window.withColumn(\"total_flag\", f.col(\"x_flag\") + f.col(\"y_flag\"))\n",
    "windows_ratio_config = [\n",
    "    {\n",
    "        \"object\": \"feature_generation.v1.core.features.windows.generate_window_ratio\",\n",
    "        \"inputs\": {\"x_flag\": \"total_flag\", \"y_flag\": \"total_flag\"},\n",
    "        \"funcs\": [\n",
    "            {\"object\": \"pyspark.sql.functions.sum\"},\n",
    "            {\"object\": \"pyspark.sql.functions.max\"},\n",
    "        ],\n",
    "        \"windows\": [\n",
    "            {\"partition_by\": [\"name\"], \"order_by\": [\"date_index\"], \"descending\": False},\n",
    "        ],\n",
    "        \"ranges_between\": [\n",
    "            [-1, -1],\n",
    "            [0, 0],\n",
    "            [1, 1],\n",
    "        ],\n",
    "        \"negative_term\": \"past\",\n",
    "        \"positive_term\": \"next\",\n",
    "        \"suffix\": \"d\",\n",
    "    },\n",
    "]\n",
    "\n",
    "df_windows_ratio = create_columns_from_config(df_window, windows_ratio_config)\n",
    "\n",
    "df_windows_ratio.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f017aa00",
   "metadata": {},
   "source": [
    "### `generate_window_delta`:\n",
    "Generates delta of window columns given a grid. Grid should contain columns,\n",
    "aggregation functions, windows, and ranges. Note that the user must ensure compatibility\n",
    " of all combinations within the grid.\n",
    "\n",
    "Core example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6834904a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as f\n",
    "from feature_generation.v1.core.features.create_column import create_columns_from_config\n",
    "from feature_generation.v1.core.features.windows import generate_window_delta\n",
    "\n",
    "windows_delta_code_config = [\n",
    "    generate_window_delta(\n",
    "        inputs=[\"x_flag\", \"y_flag\"],\n",
    "        funcs=[f.sum],\n",
    "        windows=[\n",
    "            {\"partition_by\": [\"name\"], \"order_by\": [\"date_index\"], \"descending\": False},\n",
    "        ],\n",
    "        ranges_between=[\n",
    "            [-1, -1],\n",
    "            [-1, \"UNBOUNDED FOLLOWING\"],\n",
    "        ],\n",
    "        negative_term=\"past\",\n",
    "        positive_term=\"next\",\n",
    "        suffix=\"d\",\n",
    "    )\n",
    "]\n",
    "\n",
    "df_windows_delta_code = create_columns_from_config(df_window, windows_delta_code_config)\n",
    "df_windows_delta_code.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae6887d",
   "metadata": {},
   "source": [
    "Node example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dd5c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_generation.v1.nodes.features.create_column import (\n",
    "    create_columns_from_config,\n",
    ")\n",
    "\n",
    "windows_delta_code_config = [\n",
    "    {\n",
    "        \"object\": \"feature_generation.v1.core.features.windows.generate_window_delta\",\n",
    "        \"inputs\": [\"x_flag\", \"y_flag\"],\n",
    "        \"funcs\": [\n",
    "            {\"object\": \"pyspark.sql.functions.sum\"},\n",
    "        ],\n",
    "        \"windows\": [\n",
    "            {\"partition_by\": [\"name\"], \"order_by\": [\"date_index\"], \"descending\": False},\n",
    "        ],\n",
    "        \"ranges_between\": [\n",
    "            [-1, -1],\n",
    "            [-1, \"UNBOUNDED FOLLOWING\"],\n",
    "        ],\n",
    "        \"negative_term\": \"past\",\n",
    "        \"positive_term\": \"next\",\n",
    "        \"suffix\": \"d\",\n",
    "    },\n",
    "]\n",
    "\n",
    "df_windows_delta = create_columns_from_config(df_window, windows_delta_code_config)\n",
    "\n",
    "df_windows_delta.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad221cec",
   "metadata": {},
   "source": [
    "## Interacted Features\n",
    "\n",
    "This function creates multiple columns/features based on the interactions of the\n",
    "input columns. Two/more base columns are required in order to create an\n",
    "interaction feature.\n",
    "\n",
    "We will use the following dataframe to demonstrate creating interacted features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d77499",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import (\n",
    "    ArrayType,\n",
    "    DateType,\n",
    "    IntegerType,\n",
    "    StringType,\n",
    "    StructField,\n",
    "    StructType,\n",
    ")\n",
    "\n",
    "\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"id\", IntegerType(), True),\n",
    "        StructField(\"channel_x\", IntegerType(), True),\n",
    "        StructField(\"channel_y\", IntegerType(), True),\n",
    "        StructField(\"product_1\", IntegerType(), True),\n",
    "        StructField(\"product_2\", IntegerType(), True),\n",
    "        StructField(\"key_message_a\", IntegerType(), True),\n",
    "        StructField(\"key_message_b\", IntegerType(), True),\n",
    "        StructField(\"key_message_c\", IntegerType(), True),\n",
    "    ]\n",
    ")\n",
    "data = [\n",
    "    (1, 0, 0, 0, 0, 0, 0, 0),\n",
    "    (2, 0, 1, 0, 1, 0, 0, 1),\n",
    "    (3, 1, 0, 1, 0, 0, 1, 0),\n",
    "    (4, 1, 1, 1, 1, 1, 0, 0),\n",
    "]\n",
    "df_interact = spark.createDataFrame(data, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f646138",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_interact.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9456b5",
   "metadata": {},
   "source": [
    "### `create_interaction_features`:\n",
    "Creates interaction features given a dictionary.\n",
    "\n",
    "See https://christophm.github.io/interpretable-ml-book/interaction.html for\n",
    "explanation on interaction features in the machine learning context.\n",
    "\n",
    "Code example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef13e540",
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_generation.v1.core.features.interactions import create_interaction_features\n",
    "\n",
    "df_interaction = create_interaction_features(\n",
    "    df=df_interact,\n",
    "    params_interaction=[\n",
    "        {\n",
    "            \"channel\": [\"channel_x\", \"channel_y\"],\n",
    "            \"product\": [\"product_.*\"],\n",
    "            \"key_message\": [\"key_message_.*\"],\n",
    "        }\n",
    "    ],\n",
    "    params_spine_cols=[\"id\"],\n",
    ")\n",
    "df_interaction.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff46b48",
   "metadata": {},
   "source": [
    "No node example provided as there is no change in terms of parameters provided."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
