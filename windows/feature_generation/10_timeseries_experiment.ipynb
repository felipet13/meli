{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bc5e25",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"../\")\n",
    "sys.path.insert(0, \"packages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58ff678",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if os.environ.get(\"CIRCLECI\"):\n",
    "    default_env = os.environ.get(\"CONDA_DEFAULT_ENV\")\n",
    "    os.environ[\"PYSPARK_DRIVER_PYTHON\"] = (\n",
    "        f\"/home/circleci/miniconda/envs/{default_env}/bin/python\"\n",
    "    )\n",
    "    os.environ[\"PYSPARK_PYTHON\"] = (\n",
    "        f\"/home/circleci/miniconda/envs/{default_env}/bin/python\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12daf679",
   "metadata": {},
   "source": [
    "# Time Series Experiment: Tall vs Wide\n",
    "\n",
    "\n",
    "## Purpose\n",
    "\n",
    "The aim of this experiment was to investigate the difference in the processing time for\n",
    "calculating window features on tall vs wide format on large volume of data. With the\n",
    "underlying assumption that in wide format, we need to collect the data into an array\n",
    "once then calculate many times, as opposed to tall format where each window function\n",
    "will re-order the data each time (depending on the spark optimizer).\n",
    "\n",
    "\n",
    "## Hypothesis\n",
    "\n",
    "We believe that leveraging arrays will result in faster run time because we simply\n",
    "need to collect the array once and then calculate many times. As opposed to tall format\n",
    "where we may need to shuffle and re-order the data multiple times (spark optimizer dependent).\n",
    "\n",
    "We want to find out two things:\n",
    "\n",
    "* Hypothesis A - The run time performance for the same number of rows using arrays\n",
    "  is equivalent to using native window functions in tall format.\n",
    "* Hypothesis B - The run time performance using arrays can be faster when we only\n",
    "  need certain rows. For example, we only need the window calculation value as at the end-of-month.\n",
    "  In tall format, this would require windowing all 30 days therefore 30 rows. But in\n",
    "  wide format, we only perform the calculation on 1 row.\n",
    "\n",
    "\n",
    "\n",
    "## General description\n",
    "### Tall Format\n",
    "\n",
    "When you think 'tall' think 'vertical'. Here, every row represents an\n",
    "observation belonging to a particular category. Note that in spark, row ordering\n",
    "is not preserved.\n",
    "```\n",
    "| hcp_id | time_index | value |\n",
    "|--------|------------|-------|\n",
    "| h1     | 1          | 10    |\n",
    "| h1     | 2          | 3     |\n",
    "| h1     | 3          | 4     |\n",
    "| h1     | 4          | 2     |\n",
    "| h1     | 5          | 7     |\n",
    "| h1     | 6          | 2     |\n",
    "| h2     | 1          | 4     |\n",
    "| h2     | 2          | 7     |\n",
    "| h2     | 3          | 2     |\n",
    "| h2     | 4          | 8     |\n",
    "| h2     | 5          | 2     |\n",
    "| h2     | 6          | 9     |\n",
    "```\n",
    "### Wide Format\n",
    "When you think 'wide', think 'horizontalâ€™. Here, categorical data is always grouped.\n",
    "You can think of it as a summary of long data. The order is always preserved once collected.\n",
    "\n",
    "```\n",
    "| hcp_id | time_index_array | value_array    |\n",
    "|--------|------------------|----------------|\n",
    "| h1     | [1,2,3,4,5,6]    | [10,3,4,2,7,2] |\n",
    "| h2     | [1,2,3,4,5,6]    | [4,7,2,8,2,9]  |\n",
    "```\n",
    "\n",
    "## Data Processing Logic\n",
    "\n",
    "For both formats, the pre-requisite is to join the datasets with spine\n",
    "(dataset at unit of analysis grain) in order to have an equivalent comparison\n",
    "(Hypothesis A).\n",
    "\n",
    "```\n",
    "Spine:\n",
    "| hcp_id | time_index |\n",
    "|--------|------------|\n",
    "| h1     | 1          |\n",
    "| h1     | 2          |\n",
    "| h1     | 3          |\n",
    "| h1     | 4          |\n",
    "| h1     | 5          |\n",
    "| h1     | 6          |\n",
    "| h2     | 1          |\n",
    "| h2     | 2          |\n",
    "| h2     | 3          |\n",
    "| h2     | 4          |\n",
    "| h2     | 5          |\n",
    "| h2     | 6          |\n",
    "```\n",
    "\n",
    "Let's assume the input data is at day level and we want to calculate features at the month level.\n",
    "The order of operation in both formats would be:\n",
    "\n",
    "- Tall:\n",
    "  - Partition by key(s) and aggregate over the given window range at the daily level.\n",
    "  - Filter to keep only month-end rows.\n",
    "\n",
    "![Tall_processing](images/tall_processing.png)\n",
    "\n",
    "- Wide:\n",
    "  - Filter input dataset to keep only month-end rows.\n",
    "  - An anchor (time_index from spine) is used to determine the current row. We find\n",
    "    out the anchor position in time_index_array, relative to which the window range\n",
    "    is determined.\n",
    "  - Based on the window range indices, the elements in value_array are aggregated.\n",
    "\n",
    "![Wide_processing](images/wide_processing.png)\n",
    "\n",
    "Note the ordering of the filter statement. In the wide format, we can apply the filter\n",
    "first which will massively reduce the number of rows (Hypothesis B).\n",
    "\n",
    "## Cluster Configuration\n",
    "\n",
    "For this experiment, we used a Databricks cluster with following config:\n",
    "\n",
    "```\n",
    "| Memory       | 256gb                  |\n",
    "|--------------|------------------------|\n",
    "| Cores        | 32                     |\n",
    "| Worker nodes | Scalable till 40 nodes |\n",
    "```\n",
    "\n",
    "Adjustments done for this experiment:\n",
    "\n",
    "- Spark version upgraded to 3.1.2 (9.1 LTS Databricks runtime). Slicing doesn't\n",
    "  work properly on spark 3.0.1 or less.\n",
    "- Having a lot of array columns often fill up the containers and causes executors\n",
    "  to go out of memory. Selecting only the required columns while writing the\n",
    "  dataframes helped manage the issue and decreased the execution time too.\n",
    "- Faced intermittent \"AWS insufficient Instance Capacity failure\" issues, when\n",
    "  we had all nodes as spot instances and AWS wasn't able to find any instances.\n",
    "  As a result the cluster would not spin up. To resolve this we adjusted the cluster\n",
    "  config to have 2 on-demand instances and 38 spot instances.\n",
    "\n",
    "We performed the same calculations twice, once with X amount of data (Experiment 1),\n",
    "and then again with 2X amount of data (Experiment 2).\n",
    "\n",
    "## Experiment 1\n",
    "\n",
    "### Input data\n",
    "\n",
    "Below are the details of the input data for tall and wide\n",
    "aggregation over window steps.\n",
    "\n",
    "```\n",
    "|           | Tall        | Wide        |\n",
    "|-----------|-------------|-------------|\n",
    "| Row count | 4.7 Billion | 4.7 Billion |\n",
    "```\n",
    "\n",
    "\n",
    "### Preprocessing Times\n",
    "\n",
    "```\n",
    "| Format   | Type of operation      | Time   |\n",
    "|----------|------------------------|--------|\n",
    "| Tall     | Join with spine        | 10 min |\n",
    "| Wide     | Join with spine        | 58 min |\n",
    "| Wide     | Collect list of values | 2 min  |\n",
    "| Wide eom | Filter for EOM         | 6 min  |\n",
    "```\n",
    "\n",
    "EOM: For situations where we need to calculate features over windows for end of the month,\n",
    "  the processing logic changes a bit.\n",
    "\n",
    "Tall EOM: Filter is applied after features for tall are generated.\n",
    "\n",
    "Wide EOM: Filter is applied before the feature calculation since we have all the\n",
    "  information for lookback/forward in array columns.\n",
    "\n",
    "\n",
    "### Processing Time\n",
    "\n",
    "```\n",
    "| Count of features    | Tall    | Wide      | Wide EOM |\n",
    "|----------------------|---------|-----------|----------|\n",
    "| 3 features 1 window  | 5.6 min | 11 min    | 41 sec   |\n",
    "| 3 features 2 windows | 4 min   | 12.3 min  | 1 min    |\n",
    "| 3 features 3 windows | 7.7 min | 14 min    | 1.1 min  |\n",
    "| 3 features 5 windows | 9.5 min | 13.75 min | 2.6  min |\n",
    "```\n",
    "\n",
    "\n",
    "### Postprocessing Time\n",
    "\n",
    "```\n",
    "| Format | Type of operation | Time  |\n",
    "|--------|-------------------|-------|\n",
    "| Tall   | Filter for EOM    | 2 min |\n",
    "```\n",
    "\n",
    "\n",
    "## Experiment 2\n",
    "\n",
    "### Input Data\n",
    "\n",
    "Below are the details of the input data for tall and wide\n",
    "aggregation over window steps.\n",
    "\n",
    "```\n",
    "|           | Tall        | Wide        |\n",
    "|-----------|-------------|-------------|\n",
    "| Row count | 9.4 Billion | 9.4 Billion |\n",
    "```\n",
    "\n",
    "\n",
    "### Preprocessing Time\n",
    "\n",
    "```\n",
    "| Format   | Type of operation      | Time    |\n",
    "|----------|------------------------|---------|\n",
    "| Tall     | Join with spine        | 33 min  |\n",
    "| Wide     | Join with spine        | 156 min |\n",
    "| Wide     | Collect list of values | 41 min  |\n",
    "| Wide eom | Filter for EOM         | 15 min  |\n",
    "```\n",
    "\n",
    "\n",
    "### Processing Time\n",
    "\n",
    "```\n",
    "| Count of features    | Tall     | Wide     | Wide EOM |\n",
    "|----------------------|----------|----------|----------|\n",
    "| 3 features 1 window  | 12.3 min | 25.6 min | 2.1 min  |\n",
    "| 3 features 2 windows | 23 min   | 25.2 min | 2 min    |\n",
    "| 3 features 3 windows | 40 min   | 24 min   | 2.1 min  |\n",
    "| 3 features 5 windows | 58.6 min | 25 min   | 2.4  min |\n",
    "```\n",
    "\n",
    "\n",
    "### Postprocessing Time\n",
    "\n",
    "```\n",
    "| Format | Type of operation | Time  |\n",
    "|--------|-------------------|-------|\n",
    "| Tall   | Filter for EOM    | 1 min |\n",
    "```\n",
    "\n",
    "\n",
    "## Observations\n",
    "\n",
    "1. In comparison, tall is more optimised when processing same\n",
    "  number of records but for cases where we need to calculate features for EOM/EOW etc.\n",
    "  Wide ranges from 5x ~ 10x faster.\n",
    "\n",
    "2. Considering the pre-processing time, tall process is faster but\n",
    "  processing time for wide becomes favourable as we increase the number of records and\n",
    "  windows. It is unclear why in Experiment 2 the run time does not increase linearly.\n",
    "\n",
    "3. Databricks has some instability under the hood, resulting in non-linear\n",
    "  run times in certain cases.\n",
    "\n",
    "4. The cost of generating the array structure can be considered low. Though\n",
    "  it is not clear how or why the run time would be from 2 minutes to 40 minutes. Though\n",
    "  anecdotally we've seen that `collect_list` is performant in most circumstances\n",
    "  beyond this experiment.\n",
    "\n",
    "5. The cost of joining the array structure to the spine is expensive. This makes\n",
    "  sense as the data is being replicated many times.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "### Hypothesis A\n",
    "\n",
    "For the same number of rows, the wide implementation cannot be said to be worse or better than\n",
    "tall native window functions. In Experiment 1, the implementation was a few minutes slower,\n",
    "but in Experiment 2, the implementation was ~20 minutes faster in some cases. However,\n",
    "the process to generate the array for every row in the spine would make this process\n",
    "overall slower.\n",
    "\n",
    "### Hypothesis B\n",
    "\n",
    "When we only need the window calculation for specific rows, the wide implementation is\n",
    "an order of magnitude faster. In this situation, the additional cost would be the\n",
    "collect process and the filter, the inclusion of which on larger datasets may potentially\n",
    "be worth it.\n",
    "\n",
    "\n",
    "## Recommendations\n",
    "\n",
    "When it comes to performing window calculations for every row, the tall format is generally\n",
    "still recommended. The wide format would require duplicating the data multiple times and\n",
    "in big data settings, this is not recommended.\n",
    "\n",
    "When it comes to needing the window calculation for only specific rows, the wide format\n",
    "is faster one can filter the data down the only the specific rows then perform the calculation.\n",
    "\n",
    "# Appendix\n",
    "## Data Processing Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a177e28",
   "metadata": {
    "tags": [
     "hide-input",
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.config(\"spark.ui.showConsoleProgress\", False).getOrCreate()\n",
    "import datetime\n",
    "\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "from pyspark.sql.types import (\n",
    "    IntegerType,\n",
    "    StringType,\n",
    "    StructField,\n",
    "    StructType,\n",
    "    TimestampType,\n",
    ")\n",
    "\n",
    "from feature_generation.v1.core.timeseries.array_collect import (\n",
    "    collect_array_then_interpolate,\n",
    ")\n",
    "\n",
    "\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"element_id\", StringType(), True),\n",
    "        StructField(\"reading_ts\", TimestampType(), True),\n",
    "        StructField(\"reading_val\", IntegerType(), True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "data = [\n",
    "    (\"line1\", datetime.datetime(1970, 1, 1, 5, 0), 1),\n",
    "    (\"line1\", datetime.datetime(1970, 1, 1, 5, 2), 2),\n",
    "    (\"line1\", datetime.datetime(1970, 1, 1, 5, 6), 3),\n",
    "    (\"line1\", datetime.datetime(1970, 1, 1, 5, 7), 4),\n",
    "    (\"line1\", datetime.datetime(1970, 1, 1, 5, 12), 5),\n",
    "    (\"line2\", datetime.datetime(1970, 1, 1, 5, 0), 3),\n",
    "    (\"line2\", datetime.datetime(1970, 1, 1, 5, 5), 1),\n",
    "    (\n",
    "        \"line2\",\n",
    "        datetime.datetime(1970, 1, 1, 5, 10),\n",
    "        8,\n",
    "    ),\n",
    "    (\n",
    "        \"line2\",\n",
    "        datetime.datetime(1970, 1, 1, 5, 11),\n",
    "        6,\n",
    "    ),\n",
    "    (\n",
    "        \"line2\",\n",
    "        datetime.datetime(1970, 1, 1, 5, 12),\n",
    "        7,\n",
    "    ),\n",
    "]\n",
    "\n",
    "mock_datetime_df = spark.createDataFrame(data, schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e2dd90",
   "metadata": {},
   "source": [
    "Input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d1d0ff",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from feature_generation.v1.core.timeseries.datetime import minute_index\n",
    "\n",
    "mock_datetime_df = mock_datetime_df.withColumn(\n",
    "    \"minute_index\", minute_index(\"reading_ts\")\n",
    ")\n",
    "mock_datetime_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bc5183",
   "metadata": {},
   "source": [
    "Spine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd81bc0",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"element_id\", StringType(), True),\n",
    "        StructField(\"reading_ts\", TimestampType(), True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "data = [\n",
    "    (\"line1\", datetime.datetime(1970, 1, 1, 5, 0)),\n",
    "    (\"line1\", datetime.datetime(1970, 1, 1, 5, 1)),\n",
    "    (\"line1\", datetime.datetime(1970, 1, 1, 5, 2)),\n",
    "    (\"line1\", datetime.datetime(1970, 1, 1, 5, 3)),\n",
    "    (\"line1\", datetime.datetime(1970, 1, 1, 5, 4)),\n",
    "    (\"line1\", datetime.datetime(1970, 1, 1, 5, 5)),\n",
    "    (\"line1\", datetime.datetime(1970, 1, 1, 5, 6)),\n",
    "    (\"line1\", datetime.datetime(1970, 1, 1, 5, 7)),\n",
    "    (\"line1\", datetime.datetime(1970, 1, 1, 5, 8)),\n",
    "    (\"line1\", datetime.datetime(1970, 1, 1, 5, 9)),\n",
    "    (\"line1\", datetime.datetime(1970, 1, 1, 5, 10)),\n",
    "    (\"line1\", datetime.datetime(1970, 1, 1, 5, 11)),\n",
    "    (\"line1\", datetime.datetime(1970, 1, 1, 5, 12)),\n",
    "    (\"line2\", datetime.datetime(1970, 1, 1, 5, 0)),\n",
    "    (\"line2\", datetime.datetime(1970, 1, 1, 5, 1)),\n",
    "    (\"line2\", datetime.datetime(1970, 1, 1, 5, 2)),\n",
    "    (\"line2\", datetime.datetime(1970, 1, 1, 5, 3)),\n",
    "    (\"line2\", datetime.datetime(1970, 1, 1, 5, 4)),\n",
    "    (\"line2\", datetime.datetime(1970, 1, 1, 5, 5)),\n",
    "    (\"line2\", datetime.datetime(1970, 1, 1, 5, 6)),\n",
    "    (\"line2\", datetime.datetime(1970, 1, 1, 5, 7)),\n",
    "    (\"line2\", datetime.datetime(1970, 1, 1, 5, 8)),\n",
    "    (\"line2\", datetime.datetime(1970, 1, 1, 5, 9)),\n",
    "    (\"line2\", datetime.datetime(1970, 1, 1, 5, 10)),\n",
    "    (\"line2\", datetime.datetime(1970, 1, 1, 5, 11)),\n",
    "    (\"line2\", datetime.datetime(1970, 1, 1, 5, 12)),\n",
    "]\n",
    "\n",
    "spine_df = spark.createDataFrame(data, schema)\n",
    "\n",
    "spine_df = spine_df.withColumn(\"minute_index\", minute_index(\"reading_ts\"))\n",
    "spine_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c769433",
   "metadata": {},
   "source": [
    "### Preprocessing step\n",
    "\n",
    "Tall:\n",
    "\n",
    "1. Join with spine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f44475",
   "metadata": {},
   "outputs": [],
   "source": [
    "tall_df = spine_df.join(\n",
    "    mock_datetime_df, [\"element_id\", \"reading_ts\", \"minute_index\"], \"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f21fa76",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "tall_df = tall_df.orderBy(\"element_id\", \"reading_ts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bcb0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tall_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fb18ac",
   "metadata": {},
   "source": [
    "Wide:\n",
    "1. Collect list of values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2bd01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_generation.v1.core.timeseries.array_collect import (\n",
    "    collect_array_then_interpolate,\n",
    ")\n",
    "from feature_generation.v1.core.timeseries.array_transform import interpolate_constant\n",
    "\n",
    "collected_wide_df = collect_array_then_interpolate(\n",
    "    df=mock_datetime_df,\n",
    "    order=\"minute_index\",\n",
    "    values=[\"reading_val\"],\n",
    "    groupby=\"element_id\",\n",
    "    interpolate_func=interpolate_constant,\n",
    "    constant=0,\n",
    ").drop(\"minute_index\")\n",
    "\n",
    "collected_wide_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe556dc",
   "metadata": {},
   "source": [
    "2. Join with spine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafe9a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_df = spine_df.join(collected_wide_df, [\"element_id\"], \"left\")\n",
    "\n",
    "wide_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9c2439",
   "metadata": {},
   "source": [
    "### Processing step\n",
    "\n",
    "Tall:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd7cb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_generation.v1.core.features.create_column import create_columns_from_config\n",
    "from feature_generation.v1.core.features.windows import generate_window_grid\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "tall_grid_dict = {\n",
    "    \"inputs\": [\"reading_val\"],\n",
    "    \"funcs\": [f.sum],\n",
    "    \"windows\": [\n",
    "        {\n",
    "            \"partition_by\": [\"element_id\"],\n",
    "            \"order_by\": [\"minute_index\"],\n",
    "            \"descending\": False,\n",
    "        },\n",
    "    ],\n",
    "    \"ranges_between\": [\n",
    "        [-6, -1],\n",
    "        [-3, -1],\n",
    "    ],\n",
    "    \"negative_term\": \"past\",\n",
    "    \"positive_term\": \"next\",\n",
    "    \"prefix\": \"ftr_\",\n",
    "}\n",
    "\n",
    "\n",
    "df_tall_ft = create_columns_from_config(\n",
    "    df=tall_df,\n",
    "    column_instructions=[\n",
    "        generate_window_grid(\n",
    "            **tall_grid_dict,\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "df_tall_ft.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91173fb6",
   "metadata": {},
   "source": [
    "Wide:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f20042",
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_generation.v1.core.features.windows import (\n",
    "    aggregate_over_slice_grid,\n",
    ")\n",
    "from feature_generation.v1.core.timeseries.array_aggregate import array_sum\n",
    "\n",
    "wide_grid_dict = {\n",
    "    \"inputs\": [\"reading_val_array_padded\"],\n",
    "    \"funcs\": [array_sum],\n",
    "    \"anchor_col\": \"minute_index\",\n",
    "    \"anchor_array\": \"spine_index\",\n",
    "    \"ranges_between\": [\n",
    "        [-6, -1],\n",
    "        [-3, -1],\n",
    "    ],\n",
    "    \"prefix\": \"ftr_\",\n",
    "}\n",
    "\n",
    "df_wide_ft = create_columns_from_config(\n",
    "    df=wide_df,\n",
    "    column_instructions=[\n",
    "        aggregate_over_slice_grid(\n",
    "            **wide_grid_dict,\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "\n",
    "df_wide_ft.select(\n",
    "    \"element_id\",\n",
    "    \"reading_ts\",\n",
    "    \"minute_index\",\n",
    "    \"ftr_reading_val_array_padded_array_sum_past_6_past_1\",\n",
    "    \"ftr_reading_val_array_padded_array_sum_past_3_past_1\",\n",
    ").show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
